# Override

Default settings generated by Stage 2 and individual steps can be overridden.  

## Inspecting the Configuration

Stage 2 refreshes the pillar after updating the contents.  To view the current settings, run

```
# salt '*' pillar.items
```

The output for default settings for a single minion will be similar to the following:

```
    ----------
    available_roles:
        - admin
        - mon
        - storage
        - mds
        - igw
        - rgw
        - client-cephfs
        - client-radosgw
        - client-iscsi
        - mds-nfs
        - rgw-nfs
        - master
    cluster:
        ceph
    cluster_network:
        172.16.22.0/24
    fsid:
        e08ec63c-8268-3f04-bcdb-614921e94342
    master_minion:
        admin.ceph
    mon_host:
        - 172.16.21.13
        - 172.16.21.11
        - 172.16.21.12
    mon_initial_members:
        - mon3
        - mon1
        - mon2
    public_address:
        172.16.21.11
    public_network:
        172.16.21.0/24
    roles:
        - admin
        - mon
        - mds
    time_server:
        admin.ceph
    time_service:
        ntp

```

## Stack Directory Structure

The file /srv/pillar/ceph/stack/stack.cfg defines the directory structure of the data which implies the scope of a variable.  The following files are relative to /srv/pillar/ceph/stack.

* global.yml affects all minions in the Salt cluster

* *ceph*/cluster.yml affects all minions in the the Ceph cluster named **ceph**.

* *ceph*/roles/*role*.yml affects all minions assigned a role in the  Ceph cluster named **ceph**.

* *ceph*/minions/*minion id*.yml affects the individual minion


One current exception is the ceph_conf.yml file which contains specific variables for the Ceph configuration.  This will likely become obsolete in the future.

To prevent any automation conflicting with the administrator, Stage 2 creates a parallel directory tree under /srv/pillar/ceph/stack/default.  Any files in this directory tree will **always** be overwritten.  The administrator should never make changes to files in this tree.


## Overriding Default Settings

If any data is incorrect for your environment, override it.  For instance, if the guessed cluster network is `10.0.1.0/24`, but the preferred cluster network is `172.16.22.0/24`, do the following:

* Edit the file /srv/pillar/ceph/stack/ceph/cluster.yml

* Add `cluster_network: 172.16.22.0/24`

* Save the file

To verify the change, run

```
# salt '*' saltutil.pillar_refresh
# salt '*' pillar.items
```

This can be repeated with any configuration data.  For examples, examine any of the files under /srv/pillar/ceph/stack/default.  

## Overriding Default Steps

Many of the steps have alternate defaults.  All steps have a default.sls.  An alternate default state file has the prefix *default-*.  For example, the /srv/salt/ceph/osd/default.sls calls ceph-disk to create each OSD on a storage node.  In virtual or cloud environments, disks smaller than 10G may be preferred for certain experiments.  The default-small.sls provides this functionality.

To select this alternate default, 

* Edit the file /srv/pillar/ceph/stack/ceph/roles/storage.yml

* Add `osd_init: default-small`

* Save the file

To verify the change, run

```
# salt '*' saltutil.pillar_refresh
# salt '*' pillar.items
```

Note that the name of the variable to override is always defined in the init.sls.  In this example, /srv/salt/ceph/osd/init.sls contains

```
include:
  - .{{ salt['pillar.get']('osd_init', 'default') }}
```

Salt will evaluate the Jinja expression and return either the value of *osd_init* or default.  Salt will then include a file relative to this file with an sls extension.
